{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Comparison: Clean vs. Attacked Text\n",
    "\n",
    "This notebook visualizes the impact of the **WordSwapEmbedding** attack on text quality.\n",
    "\n",
    "We compare the perplexity scores of:\n",
    "1.  **Clean Data:** Original text from the SQuAD validation set.\n",
    "2.  **Attacked Data:** The same text after being modified by TextAttack.\n",
    "\n",
    "**Hypothesis:** The attack should increase the perplexity score, shifting the distribution to the right (indicating less natural text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (relative to project root if running from there, or adjust as needed)\n",
    "clean_path = '../tests/data/measures/perplexity_data.csv'\n",
    "attacked_path = '../tests/data/measures/perplexity_attacked.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(clean_path):\n",
    "    print(f\"WARNING: {clean_path} not found. Did you run 'run_perplexity_on_clean.py'?\")\n",
    "else:\n",
    "    df_clean = pd.read_csv(clean_path)\n",
    "    print(f\"Loaded CLEAN data: {len(df_clean)} rows\")\n",
    "\n",
    "if not os.path.exists(attacked_path):\n",
    "    print(f\"WARNING: {attacked_path} not found. Did you run 'run_perplexity_on_attacked.py'?\")\n",
    "else:\n",
    "    df_attacked = pd.read_csv(attacked_path)\n",
    "    print(f\"Loaded ATTACKED data: {len(df_attacked)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals():\n",
    "    print(\"--- Clean Data Stats ---\")\n",
    "    print(df_clean['score'].describe())\n",
    "    \n",
    "if 'df_attacked' in locals():\n",
    "    print(\"\n--- Attacked Data Stats ---\")\n",
    "    print(df_attacked['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization\n",
    "\n",
    "We overlay the two histograms to visualize the shift in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals() and 'df_attacked' in locals():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot Clean Data\n",
    "    plt.hist(df_clean['score'], bins=50, alpha=0.5, label='Clean (Original)', color='blue', range=(0, 200))\n",
    "    \n",
    "    # Plot Attacked Data\n",
    "    plt.hist(df_attacked['score'], bins=50, alpha=0.5, label='Attacked (Adversarial)', color='red', range=(0, 200))\n",
    "    \n",
    "    plt.title('Perplexity Distribution: Clean vs. Attacked Text')\n",
    "    plt.xlabel('Perplexity Score (Lower is better/more natural)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.xlim(0, 200)  # Limiting x-axis to zoom in on the main distribution (remove if you want to see outliers)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot: Data missing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}